---
title: Vision Tests
description: 2 tests for image understanding capabilities
---

# Vision Tests

Vision tests verify that models can understand and describe images provided as base64-encoded data or URLs.

## Test Summary

| Test | Model | Description |
|------|-------|-------------|
| Claude Vision | claude-sonnet-4.5 | Describe image content |
| GPT-4o Vision | gpt-4o | Describe image content |

## Running Tests

```bash
cd packages/@layers/models
AI_GATEWAY_API_KEY=vck_xxx bun test gateway -t "vision"
```

## Test Implementation

### Test Image

Tests use a simple base64-encoded test image:

```typescript
// 1x1 red pixel PNG for testing
const TEST_IMAGE = 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8z8BQDwAEhQGAhKmMIQAAAABJRU5ErkJggg==';
```

### Claude Vision Test

```typescript
it('should process images with Claude', async () => {
  const { text } = await generateText({
    model: gateway('anthropic/claude-sonnet-4.5'),
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            image: TEST_IMAGE,
          },
          {
            type: 'text',
            text: 'What color is this image? Reply with just the color name.',
          },
        ],
      },
    ],
    maxTokens: 20,
  });

  expect(text.toLowerCase()).toContain('red');
});
```

### GPT-4o Vision Test

```typescript
it('should process images with GPT-4o', async () => {
  const { text } = await generateText({
    model: gateway('openai/gpt-4o'),
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            image: TEST_IMAGE,
          },
          {
            type: 'text',
            text: 'What color is this image? Reply with just the color name.',
          },
        ],
      },
    ],
    maxTokens: 20,
  });

  expect(text.toLowerCase()).toContain('red');
});
```

## Image Formats

The gateway supports multiple image formats:

| Format | Example |
|--------|---------|
| Base64 Data URL | `data:image/png;base64,iVBOR...` |
| Base64 String | `iVBORw0KGgo...` (without prefix) |
| URL | `https://example.com/image.png` |

### Using URLs

```typescript
const { text } = await generateText({
  model: gateway('anthropic/claude-sonnet-4.5'),
  messages: [
    {
      role: 'user',
      content: [
        {
          type: 'image',
          image: new URL('https://example.com/photo.jpg'),
        },
        {
          type: 'text',
          text: 'Describe this image.',
        },
      ],
    },
  ],
});
```

## Supported Models

| Model | Vision Support |
|-------|----------------|
| anthropic/claude-haiku-4.5 | Yes |
| anthropic/claude-sonnet-4.5 | Yes |
| anthropic/claude-opus-4.5 | Yes |
| openai/gpt-4o | Yes |
| openai/gpt-4o-mini | Yes |
| google/gemini-2.5-flash | Yes |
| google/gemini-2.5-pro | Yes |
| perplexity/sonar | No |
| morph/morph-v3-fast | No |

## Best Practices

1. **Use small test images** - Reduces token costs and speeds up tests
2. **Ask specific questions** - "What color?" is more testable than "Describe everything"
3. **Handle rate limits** - Vision calls are more expensive and may be rate-limited
