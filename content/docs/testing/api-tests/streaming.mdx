---
title: Streaming Tests (API)
description: 1 test for SSE streaming via the Layers API
---

# Streaming Tests (API)

Streaming tests verify that the Layers API correctly streams responses using Server-Sent Events (SSE).

## Test Summary

| Test | Model | Description |
|------|-------|-------------|
| Claude Streaming | claude-haiku-4.5 | Stream text chunks via SSE |

## Running Tests

```bash
cd packages/@layers/models
LAYERS_API_URL=https://layers.hustletogether.com \
LAYERS_API_KEY=lyr_live_xxx \
bun test layers-api -t "stream"
```

## Test Implementation

### Streaming Test

```typescript
it('should stream responses via API', async () => {
  const response = await fetch(`${LAYERS_API_URL}/api/v1/chat`, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${LAYERS_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'anthropic/claude-haiku-4.5',
      messages: [
        { role: 'user', content: 'Count from 1 to 5.' }
      ],
      stream: true,
      max_tokens: 50,
    }),
  });

  expect(response.status).toBe(200);
  expect(response.headers.get('content-type')).toContain('text/event-stream');

  const reader = response.body!.getReader();
  const decoder = new TextDecoder();
  const chunks: string[] = [];

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const text = decoder.decode(value);
    const lines = text.split('\n');

    for (const line of lines) {
      if (line.startsWith('data: ') && line !== 'data: [DONE]') {
        const data = JSON.parse(line.slice(6));
        if (data.choices[0].delta.content) {
          chunks.push(data.choices[0].delta.content);
        }
      }
    }
  }

  expect(chunks.length).toBeGreaterThan(1);
  const fullText = chunks.join('');
  expect(fullText).toContain('1');
  expect(fullText).toContain('5');
});
```

## Request Format

```json
{
  "model": "anthropic/claude-haiku-4.5",
  "messages": [
    { "role": "user", "content": "Hello!" }
  ],
  "stream": true,
  "max_tokens": 100
}
```

## SSE Response Format

```
data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","created":1705612800,"model":"anthropic/claude-haiku-4.5","choices":[{"index":0,"delta":{"role":"assistant"},"finish_reason":null}]}

data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","created":1705612800,"model":"anthropic/claude-haiku-4.5","choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}

data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","created":1705612800,"model":"anthropic/claude-haiku-4.5","choices":[{"index":0,"delta":{"content":"!"},"finish_reason":null}]}

data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","created":1705612800,"model":"anthropic/claude-haiku-4.5","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

## Chunk Structure

```typescript
interface StreamChunk {
  id: string;
  object: "chat.completion.chunk";
  created: number;
  model: string;
  choices: [{
    index: number;
    delta: {
      role?: "assistant";
      content?: string;
    };
    finish_reason: "stop" | "length" | "tool_calls" | null;
  }];
}
```

## Supported Models

All language models support streaming:

| Model | Streaming |
|-------|-----------|
| anthropic/claude-* | Yes |
| openai/gpt-* | Yes |
| google/gemini-* | Yes |
| perplexity/sonar | Yes |
| morph/morph-* | Yes |

## Usage Example

```typescript
async function streamChat(prompt: string) {
  const response = await fetch('https://api.layers.dev/v1/chat', {
    method: 'POST',
    headers: {
      'Authorization': 'Bearer lyr_live_xxx',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'anthropic/claude-haiku-4.5',
      messages: [{ role: 'user', content: prompt }],
      stream: true,
      max_tokens: 500,
    }),
  });

  const reader = response.body!.getReader();
  const decoder = new TextDecoder();

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const text = decoder.decode(value);
    const lines = text.split('\n');

    for (const line of lines) {
      if (line.startsWith('data: ') && line !== 'data: [DONE]') {
        try {
          const data = JSON.parse(line.slice(6));
          const content = data.choices[0].delta.content;
          if (content) {
            process.stdout.write(content);
          }
        } catch (e) {
          // Skip invalid JSON
        }
      }
    }
  }
}
```

## React Example

```typescript
import { useState } from 'react';

function ChatStream() {
  const [response, setResponse] = useState('');

  async function handleSubmit(prompt: string) {
    setResponse('');

    const res = await fetch('/api/chat', {
      method: 'POST',
      body: JSON.stringify({ prompt, stream: true }),
    });

    const reader = res.body!.getReader();
    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const text = decoder.decode(value);
      // Parse SSE and update state
      for (const line of text.split('\n')) {
        if (line.startsWith('data: ') && line !== 'data: [DONE]') {
          const data = JSON.parse(line.slice(6));
          const content = data.choices[0]?.delta?.content;
          if (content) {
            setResponse(prev => prev + content);
          }
        }
      }
    }
  }

  return <div>{response}</div>;
}
```

## Error Handling

```typescript
try {
  const response = await fetch(/* ... */);

  if (!response.ok) {
    const error = await response.json();
    throw new Error(error.error.message);
  }

  const reader = response.body!.getReader();
  // Process stream...

} catch (error) {
  if (error.name === 'AbortError') {
    console.log('Stream aborted');
  } else {
    console.error('Stream error:', error);
  }
}
```

## Abort Streaming

```typescript
const controller = new AbortController();

const response = await fetch(url, {
  signal: controller.signal,
  // ...
});

// Cancel the stream
controller.abort();
```
